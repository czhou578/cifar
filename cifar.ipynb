{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# Define Dataset and Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "cifar_train = datasets.CIFAR100(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "cifar_classes = cifar_train.classes\n",
    "\n",
    "class CIFAR100Dataset(Dataset):\n",
    "    def __init__(self, cifar_dataset):\n",
    "        self.cifar_dataset = cifar_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cifar_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.cifar_dataset[idx]\n",
    "        return image, label\n",
    "# Define Model Architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=12, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "    \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 100)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# Set Up Training Components\n",
    "wrapped = CIFAR100Dataset(cifar_train)\n",
    "train_loader = DataLoader(\n",
    "    wrapped, \n",
    "    batch_size=512, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "mlp = MLP().to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=0.001)\n",
    "scaler = GradScaler()\n",
    "# Train the Model\n",
    "losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(30):\n",
    "    epoch_start = time.time()\n",
    "    print(f'Starting Epoch {epoch+1}')\n",
    "\n",
    "    current_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = mlp(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        utils.clip_grad_norm_(mlp.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = current_loss / num_batches\n",
    "    losses.append(avg_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f'Epoch {epoch+1} finished - Loss: {avg_loss:.4f} - Time: {epoch_time:.1f}s')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time/60:.1f} minutes\")\n",
    "# Plot Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ad87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(losses) + 1), losses, marker='o', label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
